{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:23:01.213473Z",
          "iopub.status.busy": "2025-05-12T19:23:01.213224Z",
          "iopub.status.idle": "2025-05-12T19:23:04.497051Z",
          "shell.execute_reply": "2025-05-12T19:23:04.496060Z",
          "shell.execute_reply.started": "2025-05-12T19:23:01.213452Z"
        },
        "id": "pR_A_pj2ojzv",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q pymupdf PyPDF2 transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0d423f7bd2bf47efb4952f86007c8524"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-05-12T19:23:04.498850Z",
          "iopub.status.busy": "2025-05-12T19:23:04.498276Z",
          "iopub.status.idle": "2025-05-12T19:23:04.517143Z",
          "shell.execute_reply": "2025-05-12T19:23:04.516087Z",
          "shell.execute_reply.started": "2025-05-12T19:23:04.498808Z"
        },
        "id": "umJ3BmAsojzv",
        "outputId": "4e5e82a1-baf4-4b71-d112-4613550ccedb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa34480a831f49aa860a9b79de3eee1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:24:59.620662Z",
          "iopub.status.busy": "2025-05-12T19:24:59.620394Z",
          "iopub.status.idle": "2025-05-12T19:27:05.234104Z",
          "shell.execute_reply": "2025-05-12T19:27:05.232712Z",
          "shell.execute_reply.started": "2025-05-12T19:24:59.620638Z"
        },
        "id": "-QK4o4OBojzw",
        "outputId": "3c5f99c8-be66-4fb1-bda3-c01d38f52afd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q mistral_inference langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "dcb77aa902544b658a60502af70600ec"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-05-12T19:23:14.406543Z",
          "iopub.status.busy": "2025-05-12T19:23:14.406227Z",
          "iopub.status.idle": "2025-05-12T19:24:59.610984Z",
          "shell.execute_reply": "2025-05-12T19:24:59.610308Z",
          "shell.execute_reply.started": "2025-05-12T19:23:14.406517Z"
        },
        "id": "f2F40WX6ojzw",
        "outputId": "4bda7e57-cd03-4b69-8126-cbcde4c53068",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "gemini-1.5-flash-latest is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/gemini-1.5-flash-latest/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1643\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1640\u001b[0m ):\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1645\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1531\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1531\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1448\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m hf_raise_for_status(r)\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
            "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68272d89-35612623216685b40667706c;6ade22f8-2759-4382-849f-88b20c82b625)\n\nRepository Not Found for url: https://huggingface.co/gemini-1.5-flash-latest/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:946\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    945\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    948\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:778\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    777\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 778\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "File \u001b[1;32mc:\\Users\\laptop\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mOSError\u001b[0m: gemini-1.5-flash-latest is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "model_name = \"gemini-1.5-flash-latest\" \n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:24:59.614682Z",
          "iopub.status.busy": "2025-05-12T19:24:59.613939Z",
          "iopub.status.idle": "2025-05-12T19:24:59.619552Z",
          "shell.execute_reply": "2025-05-12T19:24:59.618674Z",
          "shell.execute_reply.started": "2025-05-12T19:24:59.614639Z"
        },
        "id": "8e3eNZsKojzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:27:05.236381Z",
          "iopub.status.busy": "2025-05-12T19:27:05.236043Z",
          "iopub.status.idle": "2025-05-12T19:27:06.009655Z",
          "shell.execute_reply": "2025-05-12T19:27:06.008954Z",
          "shell.execute_reply.started": "2025-05-12T19:27:05.236355Z"
        },
        "id": "jVuJTrfxojzw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "import PyPDF2\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_xaCDOmojzx"
      },
      "source": [
        "# 1. Explain the research in EASY Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WirG9TKOojzx"
      },
      "source": [
        "## 1. SECTION EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:27:14.590575Z",
          "iopub.status.busy": "2025-05-12T19:27:14.590209Z",
          "iopub.status.idle": "2025-05-12T19:27:14.598451Z",
          "shell.execute_reply": "2025-05-12T19:27:14.597604Z",
          "shell.execute_reply.started": "2025-05-12T19:27:14.590551Z"
        },
        "id": "NDg6daHuojzx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "SECTION_PATTERNS = {\n",
        "    \"abstract\": r'\\babstract\\b',\n",
        "    \"introduction\": r'\\bintroduction\\b',\n",
        "    \"methodology\": r'\\b(methodology|methods|materials and methods)\\b',\n",
        "    \"results\": r'\\b(results|findings)\\b',\n",
        "    \"conclusion\": r'\\b(conclusion|conclusions|discussion and conclusion)\\b'\n",
        "}\n",
        "SECTION_REGEX = {k: re.compile(v, re.IGNORECASE) for k, v in SECTION_PATTERNS.items()}\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "def extract_sections(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    combined_pattern = r'|'.join(f'(?P<{k}>{v})' for k, v in SECTION_PATTERNS.items())\n",
        "    section_header_regex = re.compile(combined_pattern, re.IGNORECASE)\n",
        "\n",
        "    sections = {}\n",
        "    matches = list(section_header_regex.finditer(text))\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        section_name = match.lastgroup\n",
        "        start = match.start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "        content = text[start:end]\n",
        "        sections[section_name] = content.strip()\n",
        "\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wd6wQ2kojzy"
      },
      "source": [
        "## 2. EXPLANATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:29:39.096095Z",
          "iopub.status.busy": "2025-05-12T19:29:39.095463Z",
          "iopub.status.idle": "2025-05-12T19:29:39.099889Z",
          "shell.execute_reply": "2025-05-12T19:29:39.099107Z",
          "shell.execute_reply.started": "2025-05-12T19:29:39.096061Z"
        },
        "id": "1FvKNJbBojzy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_json_block(text):\n",
        "    pattern = r'```json\\s*(.*?)\\s*```'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "\n",
        "    return f\"```json\\n{matches[-1]}\\n```\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:29:37.815632Z",
          "iopub.status.busy": "2025-05-12T19:29:37.814940Z",
          "iopub.status.idle": "2025-05-12T19:29:37.820824Z",
          "shell.execute_reply": "2025-05-12T19:29:37.820109Z",
          "shell.execute_reply.started": "2025-05-12T19:29:37.815605Z"
        },
        "id": "vdUNyHrmojzy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def Simplify_research_part(content):\n",
        "    explanation_schema = ResponseSchema(\n",
        "        name=\"explanation\",\n",
        "        description=\"A simplified and clear explanation of the provided research text.\"\n",
        "    )\n",
        "    output_parser = StructuredOutputParser.from_response_schemas([explanation_schema])\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    prompt_template = \"\"\"\\\n",
        "    You are a helpful assistant that explains research papers in a simple way.\n",
        "\n",
        "    TASK:\n",
        "    - Read the provided research content carefully.\n",
        "\n",
        "    - Return a clear and simplified explanation in one paragraph using plain, accurate language.\n",
        "\n",
        "    - Avoid adding any information that is not in the original content.\n",
        "\n",
        "    - The goal is to help anyone understand this part, even without a research background, while staying true to the original meaning.\n",
        "\n",
        "    Content:\n",
        "    {content}\n",
        "\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"content\", \"format_instructions\"]\n",
        "    )\n",
        "\n",
        "    formatted_prompt = prompt.format(content=content, format_instructions=format_instructions)\n",
        "\n",
        "    response = generate_text(formatted_prompt, max_length=2000, num_return_sequences=1)\n",
        "    final_response = extract_json_block(response[0])\n",
        "    output = output_parser.parse(final_response)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHr5Omdlojzy"
      },
      "source": [
        "## 3. MAIN WORKFLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:29:40.464547Z",
          "iopub.status.busy": "2025-05-12T19:29:40.464188Z",
          "iopub.status.idle": "2025-05-12T19:29:40.470655Z",
          "shell.execute_reply": "2025-05-12T19:29:40.469823Z",
          "shell.execute_reply.started": "2025-05-12T19:29:40.464522Z"
        },
        "id": "v6uxRmjyojzy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def main(pdf_path):\n",
        "    full_text = extract_text_from_pdf(pdf_path)\n",
        "    sections = extract_sections(full_text)\n",
        "    results = {}\n",
        "\n",
        "    for key in ['abstract', 'introduction', 'methodology', 'results', 'conclusion']:\n",
        "        print(f\"\\n\\n===== {key.capitalize()} =====\\n\")\n",
        "        section_text = sections.get(key, \"\")\n",
        "        if section_text:\n",
        "            # print(section_text[:500] + \"...\" if len(section_text) > 500 else section_text)\n",
        "            try:\n",
        "                simplified = Simplify_research_part(section_text)\n",
        "                results[key] = simplified['explanation']\n",
        "                # print(f\"\\nSimplified Explanation of {key.capitalize()}:\\n{simplified}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not simplify {key}: {str(e)}\")\n",
        "        else:\n",
        "            print(\"Section not found.\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A68gkI45ojzz"
      },
      "source": [
        "## Explain all research paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T19:39:54.124349Z",
          "iopub.status.busy": "2025-05-12T19:39:54.123661Z",
          "iopub.status.idle": "2025-05-12T19:40:45.685303Z",
          "shell.execute_reply": "2025-05-12T19:40:45.684554Z",
          "shell.execute_reply.started": "2025-05-12T19:39:54.124323Z"
        },
        "id": "Z4VpXr5Sojzz",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "===== Abstract =====\n",
            "\n",
            "Could not simplify abstract: name 'model' is not defined\n",
            "\n",
            "\n",
            "===== Introduction =====\n",
            "\n",
            "Could not simplify introduction: name 'model' is not defined\n",
            "\n",
            "\n",
            "===== Methodology =====\n",
            "\n",
            "Could not simplify methodology: name 'model' is not defined\n",
            "\n",
            "\n",
            "===== Results =====\n",
            "\n",
            "Could not simplify results: name 'model' is not defined\n",
            "\n",
            "\n",
            "===== Conclusion =====\n",
            "\n",
            "Could not simplify conclusion: name 'model' is not defined\n"
          ]
        }
      ],
      "source": [
        "pdf_file_path = r\"arxiv_pdfs\\0704.1020.pdf\"\n",
        "answer = main(pdf_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying with GEMINI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import re\n",
        "import PyPDF2\n",
        "import google.generativeai as genai\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "\n",
        "load_dotenv()  # Loads variables from .env into environment\n",
        "\n",
        "# Get API key for Gemini\n",
        "GEMINI_API_KEY = os.getenv(\"API_KEY\")  # Make sure to set this in your .env file\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize Gemini model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    \"\"\"Generate text using Gemini model\"\"\"\n",
        "    try:\n",
        "        # Configure generation parameters\n",
        "        generation_config = genai.types.GenerationConfig(\n",
        "            max_output_tokens=max_length * 4,  # Gemini uses tokens differently than words\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        \n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        \n",
        "        return [response.text]\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating text: {e}\")\n",
        "        return [f\"Error: {str(e)}\"]\n",
        "\n",
        "# the section patterns to identify different parts of a research paper\n",
        "SECTION_PATTERNS = {\n",
        "    \"abstract\": r'\\babstract\\b',\n",
        "    \"introduction\": r'\\bintroduction\\b',\n",
        "    \"methodology\": r'\\b(methodology|methods|materials and methods)\\b',\n",
        "    \"results\": r'\\b(results|findings)\\b',\n",
        "    \"conclusion\": r'\\b(conclusion|conclusions|discussion and conclusion)\\b'\n",
        "}\n",
        "\n",
        "# Compile the regex patterns for section headers\n",
        "SECTION_REGEX = {k: re.compile(v, re.IGNORECASE) for k, v in SECTION_PATTERNS.items()}\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "def extract_sections(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    combined_pattern = r'|'.join(f'(?P<{k}>{v})' for k, v in SECTION_PATTERNS.items())\n",
        "    section_header_regex = re.compile(combined_pattern, re.IGNORECASE)\n",
        "\n",
        "    sections = {}\n",
        "    matches = list(section_header_regex.finditer(text))\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        section_name = match.lastgroup\n",
        "        start = match.start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "        content = text[start:end]\n",
        "        sections[section_name] = content.strip()\n",
        "\n",
        "    return sections\n",
        "\n",
        "def extract_json_block(text):\n",
        "    \"\"\"Extract JSON from markdown code blocks\"\"\"\n",
        "    pattern = r'```json\\s*(.*?)\\s*```'\n",
        "    matches = re.findall(pattern, text, re.DOTALL)\n",
        "    \n",
        "    if matches:\n",
        "        return f\"```json\\n{matches[-1]}\\n```\"\n",
        "    \n",
        "    # If no markdown json found, try to find JSON directly\n",
        "    try:\n",
        "        # Look for JSON pattern in the text\n",
        "        json_pattern = r'\\{[^}]*\"explanation\"[^}]*\\}'\n",
        "        json_match = re.search(json_pattern, text, re.DOTALL)\n",
        "        if json_match:\n",
        "            return f\"```json\\n{json_match.group()}\\n```\"\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # If still no JSON found, wrap the entire response as explanation\n",
        "    return f'```json\\n{{\"explanation\": \"{text.strip()}\"}}\\n```'\n",
        "\n",
        "def Simplify_research_part(content):\n",
        "    \"\"\"Simplify research content using Gemini\"\"\"\n",
        "    explanation_schema = ResponseSchema(\n",
        "        name=\"explanation\",\n",
        "        description=\"A simplified and clear explanation of the provided research text.\"\n",
        "    )\n",
        "    output_parser = StructuredOutputParser.from_response_schemas([explanation_schema])\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    prompt_template = \"\"\"\\\n",
        "    You are a helpful assistant that explains research papers in a simple way.\n",
        "\n",
        "    TASK:\n",
        "    - Read the provided research content carefully.\n",
        "    - Return a clear and simplified explanation in one paragraph using plain, accurate language.\n",
        "    - Avoid adding any information that is not in the original content.\n",
        "    - The goal is to help anyone understand this part, even without a research background, while staying true to the original meaning.\n",
        "    - Format your response as JSON with the key \"explanation\".\n",
        "\n",
        "    Content:\n",
        "    {content}\n",
        "\n",
        "    Please provide your response in the following JSON format:\n",
        "    {{\n",
        "        \"explanation\": \"Your simplified explanation here\"\n",
        "    }}\n",
        "\n",
        "    {format_instructions}\n",
        "    \"\"\"\n",
        "    \n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"content\", \"format_instructions\"]\n",
        "    )\n",
        "\n",
        "    formatted_prompt = prompt.format(content=content, format_instructions=format_instructions)\n",
        "\n",
        "    # Generate response with increased max_length for better explanations\n",
        "    response = generate_text(formatted_prompt, max_length=500, temperature=0.7)\n",
        "    \n",
        "    # Extract JSON block\n",
        "    final_response = extract_json_block(response[0])\n",
        "    \n",
        "    try:\n",
        "        # Parse the output\n",
        "        output = output_parser.parse(final_response)\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        # If parsing fails, return a basic structure\n",
        "        print(f\"Parsing error: {e}\")\n",
        "        # Try to extract explanation manually\n",
        "        try:\n",
        "            # Clean up the response and extract explanation\n",
        "            cleaned_response = response[0].strip()\n",
        "            if cleaned_response.startswith('\"') and cleaned_response.endswith('\"'):\n",
        "                cleaned_response = cleaned_response[1:-1]\n",
        "            return {\"explanation\": cleaned_response}\n",
        "        except:\n",
        "            return {\"explanation\": \"Unable to simplify this section\"}\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Process the PDF content to extract text and summarize it using Gemini.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        full_text = extract_text_from_pdf(pdf_path)\n",
        "        sections = extract_sections(full_text)\n",
        "        results = {}\n",
        "\n",
        "        for key in ['abstract', 'introduction', 'methodology', 'results', 'conclusion']:\n",
        "            print(f\"\\n\\n===== {key.capitalize()} =====\\n\")\n",
        "            section_text = sections.get(key, \"\")\n",
        "            if section_text:\n",
        "                # Limit section text length to avoid token limits\n",
        "                if len(section_text) > 3000:\n",
        "                    section_text = section_text[:3000] + \"...\"\n",
        "                \n",
        "                try:\n",
        "                    simplified = Simplify_research_part(section_text)\n",
        "                    results[key] = simplified['explanation']\n",
        "                    print(f\"âœ… Successfully simplified {key}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Could not simplify {key}: {str(e)}\")\n",
        "                    # Add error handling to continue processing other sections\n",
        "                    results[key] = f\"Error processing {key}: {str(e)}\"\n",
        "            else:\n",
        "                print(\"Section not found.\")\n",
        "                results[key] = \"Section not found in the document.\"\n",
        "\n",
        "        return results\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Test function to verify Gemini is working\n",
        "def test_gemini():\n",
        "    \"\"\"Test if Gemini is properly configured\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(\"Please respond with 'Gemini is working correctly!'\")\n",
        "        print(f\"âœ… Gemini test successful: {response.text}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Gemini test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Additional helper function for better error handling\n",
        "def process_pdf_with_fallback(pdf_path):\n",
        "    \"\"\"\n",
        "    Process PDF with fallback mechanisms\n",
        "    \"\"\"\n",
        "    # First test if Gemini is working\n",
        "    if not test_gemini():\n",
        "        return {\"error\": \"Gemini API is not properly configured\"}\n",
        "    \n",
        "    return process_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Gemini test successful: Gemini is working correctly!\n",
            "\n",
            "\n",
            "\n",
            "===== Abstract =====\n",
            "\n",
            "âœ… Successfully simplified abstract\n",
            "\n",
            "\n",
            "===== Introduction =====\n",
            "\n",
            "âœ… Successfully simplified introduction\n",
            "\n",
            "\n",
            "===== Methodology =====\n",
            "\n",
            "âœ… Successfully simplified methodology\n",
            "\n",
            "\n",
            "===== Results =====\n",
            "\n",
            "âœ… Successfully simplified results\n",
            "\n",
            "\n",
            "===== Conclusion =====\n",
            "\n",
            "âœ… Successfully simplified conclusion\n"
          ]
        }
      ],
      "source": [
        "pdf_file_path = r\"arxiv_pdfs\\0704.1020.pdf\"\n",
        "answer = process_pdf_with_fallback(pdf_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "===== Final Results =====\n",
            "\n",
            "abstract:\n",
            "This research explores finding the shortest path in a graph where edge weights change unpredictably.  An algorithm is developed that, over many rounds, finds paths whose total weight is close to the best path possible in hindsight.  The algorithm's performance is proven to be efficient, scaling well with the number of rounds and edges.  The research also extends the algorithm to scenarios with limited information about edge weights and to situations where the best path itself changes over time.\n",
            "\n",
            "introduction:\n",
            "This research explores a decision-making process where someone repeatedly makes choices, each resulting in a loss depending on the environment's response.  The goal is to minimize total loss over time without making assumptions about how the losses are created.  The research focuses on performing as well as the best among a group of other decision-makers (experts), aiming to match their average loss in the long run.\n",
            "\n",
            "methodology:\n",
            "Existing methods for finding the best path in a network (the multi-armed bandit problem) either don't converge quickly enough or become much less efficient as the network grows larger.  A new algorithm is presented that performs better, especially when the best path can change over time (but the changes must be relatively infrequent).  A simpler algorithm with slightly worse performance is also described for a more limited version of the problem where only the total cost of each path is known.\n",
            "\n",
            "results:\n",
            "This text lists several research papers on computational learning theory, focusing on algorithms for online decision-making, geometric optimization, and prediction strategies.  Specific topics covered include the weighted majority algorithm, efficient algorithms for online decision problems, and aggregating strategies.  The papers were published in various conferences and journals between 1990 and 2004.\n",
            "\n",
            "conclusion:\n",
            "This research studied finding the shortest path in a network where information about the network is limited and conditions can change unpredictably, like in mobile or secure networks.  They developed efficient methods to find these paths even when edge conditions (like delays) vary arbitrarily and depend on past choices.  These methods' performance, compared to the best unchanging path, improves over time, with error decreasing proportionally to the square root of the time horizon and increasing only polynomially with the network size.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the final results   \n",
        "print(\"\\n\\n===== Final Results =====\\n\")\n",
        "for section, content in answer.items():\n",
        "    print(f\"{section}:\\n{content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7310316,
          "sourceId": 11649185,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
